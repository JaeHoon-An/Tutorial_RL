{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b268db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1556c05e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.04540147 -0.01949354  0.01132249 -0.0464231 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jaehoon/anaconda3/envs/ML_env/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:97: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "# observed value\n",
    "obs = env.reset()\n",
    "print(obs) # position, velocity, angular position, angular velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "401edfae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pygame render\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1154320a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the number of possible actions\n",
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16dfc07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "action = 1\n",
    "obs, reward, done, info = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d771a8b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.03714275  0.21301685  0.03923215 -0.29308167]\n",
      "1.0\n",
      "False\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "print(obs)\n",
    "print(reward)\n",
    "print(done)\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80deb5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_policy(obs):\n",
    "    angle = obs[2]\n",
    "    if(angle < 0):\n",
    "        ret = 0\n",
    "    else:\n",
    "        ret = 1\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "556be924",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m obs, reward, done, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m      8\u001b[0m episode_rewards \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[0;32m----> 9\u001b[0m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ML_env/lib/python3.8/site-packages/gym/core.py:343\u001b[0m, in \u001b[0;36mWrapper.render\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    342\u001b[0m     \u001b[38;5;124;03m\"\"\"Renders the environment with kwargs.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ML_env/lib/python3.8/site-packages/gym/wrappers/order_enforcing.py:51\u001b[0m, in \u001b[0;36mOrderEnforcing.render\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_disable_render_order_enforcing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call `env.render()` before calling `env.reset()`, if this is a intended action, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     49\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset `disable_render_order_enforcing=True` on the OrderEnforcer wrapper.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     50\u001b[0m     )\n\u001b[0;32m---> 51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ML_env/lib/python3.8/site-packages/gym/wrappers/env_checker.py:57\u001b[0m, in \u001b[0;36mPassiveEnvChecker.render\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m passive_env_render_check(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ML_env/lib/python3.8/site-packages/gym/envs/classic_control/cartpole.py:273\u001b[0m, in \u001b[0;36mCartPoleEnv.render\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    272\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mevent\u001b[38;5;241m.\u001b[39mpump()\n\u001b[0;32m--> 273\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrender_fps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    274\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mflip()\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "totals = []\n",
    "for episode in range(500):\n",
    "    episode_rewards = 0\n",
    "    obs = env.reset()\n",
    "    for step in range(200):\n",
    "        action = simple_policy(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        episode_rewards += reward\n",
    "        env.render()\n",
    "        if done:\n",
    "            break\n",
    "    totals.append(episode_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8344dd3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean :  41.716 std :  9.034342477457892 min :  24.0 max :  66.0\n"
     ]
    }
   ],
   "source": [
    "print('mean : ', np.mean(totals), 'std : ', np.std(totals), 'min : ', np.min(totals), 'max : ', np.max(totals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "904e96d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 4\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(5, activation=\"elu\", input_shape=[n_inputs]),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "87589e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_one_step(env, obs, model, loss_fn):\n",
    "    with tf.GradientTape() as tape:\n",
    "        left_proba = model(obs[np.newaxis])\n",
    "        action = (tf.random.uniform([1, 1]) > left_proba)\n",
    "        y_target = tf.constant([[1.]]) - tf.cast(action, tf.float32)\n",
    "        loss = tf.reduce_mean(loss_fn(y_target, left_proba))\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    obs, reward, done, info = env.step(int(action[0, 0].numpy()))\n",
    "    return obs, reward, done, grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "96eafc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_multiple_episodes(env, n_episodes, n_max_steps, model, loss_fn):\n",
    "    all_rewards = []\n",
    "    all_grads = []\n",
    "    for episode in range(n_episodes):\n",
    "        current_rewards = []\n",
    "        current_grads = []\n",
    "        obs = env.reset()\n",
    "        for step in range(n_max_steps):\n",
    "            obs, reward, done, grads = play_one_step(env, obs, model, loss_fn)\n",
    "            current_rewards.append(reward)\n",
    "            current_grads.append(grads)\n",
    "            if done:\n",
    "                break\n",
    "        all_rewards.append(current_rewards)\n",
    "        all_grads.append(current_grads)\n",
    "    return all_rewards, all_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e84e90a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(rewards, discount_rate):\n",
    "    discounted = np.array(rewards)\n",
    "    for step in range(len(rewards) - 2, -1, -1):\n",
    "        discounted[step] += discounted[step + 1] * discount_rate\n",
    "    return discounted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4f32aa4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_and_normalize_rewards(all_rewards, discount_rate):\n",
    "    all_discounted_rewards = [discount_rewards(rewards, discount_rate)\n",
    "                              for rewards in all_rewards]\n",
    "    flat_rewards = np.concatenate(all_discounted_rewards)\n",
    "    reward_mean = flat_rewards.mean()\n",
    "    reward_std = flat_rewards.std()\n",
    "    return [(discounted_rewards - reward_mean) / reward_std\n",
    "            for discounted_rewards in all_discounted_rewards]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bb73733f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-22, -40, -50])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discount_rewards([10, 0, -50], discount_rate=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ed315117",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-0.28435071, -0.86597718, -1.18910299]),\n",
       " array([1.26665318, 1.0727777 ])]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discount_and_normalize_rewards([[10,0,-50],[10, 20]], discount_rate=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "16855d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 100\n",
    "n_episodes_per_update = 10\n",
    "n_max_steps = 400\n",
    "discount_rate = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fccf0380",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(learning_rate=0.007)\n",
    "loss_fn = keras.losses.binary_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4a330156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 99, mean rewards: 381.0"
     ]
    }
   ],
   "source": [
    "for iteration in range(n_iterations):\n",
    "    all_rewards, all_grads = play_multiple_episodes(\n",
    "        env, n_episodes_per_update, n_max_steps, model, loss_fn)\n",
    "    total_rewards = sum(map(sum, all_rewards))                     # Not shown in the book\n",
    "    print(\"\\rIteration: {}, mean rewards: {:.1f}\".format(          # Not shown\n",
    "        iteration, total_rewards / n_episodes_per_update), end=\"\") # Not shown\n",
    "    all_final_rewards = discount_and_normalize_rewards(all_rewards,\n",
    "                                                       discount_rate)\n",
    "    all_mean_grads = []\n",
    "    for var_index in range(len(model.trainable_variables)):\n",
    "        mean_grads = tf.reduce_mean(\n",
    "            [final_reward * all_grads[episode_index][step][var_index]\n",
    "             for episode_index, final_rewards in enumerate(all_final_rewards)\n",
    "                 for step, final_reward in enumerate(final_rewards)], axis=0)\n",
    "        all_mean_grads.append(mean_grads)\n",
    "    optimizer.apply_gradients(zip(all_mean_grads, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c603f95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: models/1st_learned_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-08 16:57:03.729589: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    }
   ],
   "source": [
    "model.save('models/1st_learned_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "606039fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_trained_model(n_max_steps=400):\n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "    obs = env.reset()\n",
    "    for step in range(n_max_steps):\n",
    "        left_proba = model.predict(obs.reshape(1, -1))\n",
    "        action = int(np.random.rand() > left_proba)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        env.render()\n",
    "        if done:\n",
    "            break\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e137d0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_trained_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8ab451af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(4, 5), dtype=float32, numpy=\n",
       " array([[ 1.9074937e-04, -3.2367632e-05, -4.1303618e-04, -4.2168500e-05,\n",
       "         -5.3802072e-05],\n",
       "        [ 1.3819337e-03, -2.3294112e-04, -2.0992579e-03, -2.3773212e-03,\n",
       "         -2.1802902e-03],\n",
       "        [-1.0039198e-04,  1.9278592e-05,  1.1088847e-05,  7.3342392e-04,\n",
       "          6.4214121e-04],\n",
       "        [-1.7743259e-03,  3.0406620e-04,  1.9464429e-03,  5.2335402e-03,\n",
       "          4.6678316e-03]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5,), dtype=float32, numpy=\n",
       " array([ 0.00121662, -0.00022681, -0.00372062,  0.00425246,  0.00362416],\n",
       "       dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 0.00529886],\n",
       "        [ 0.00145386],\n",
       "        [-0.00105939],\n",
       "        [-0.00275677],\n",
       "        [-0.00192604]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.01368127], dtype=float32)>]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_mean_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6f3485b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 2.1791304 ,  2.02380075,  1.86847111,  1.71314147,  1.55781182,\n",
       "         1.40248218,  1.24715254,  1.0918229 ,  0.93649325,  0.78116361,\n",
       "         0.62583397,  0.47050432,  0.31517468,  0.15984504,  0.0045154 ,\n",
       "        -0.15081425, -0.30614389, -0.46147353, -0.61680317, -0.77213282,\n",
       "        -0.92746246, -1.0827921 , -1.23812175, -1.39345139]),\n",
       " array([ 0.31517468,  0.15984504,  0.0045154 , -0.15081425, -0.30614389,\n",
       "        -0.46147353, -0.61680317, -0.77213282, -0.92746246, -1.0827921 ,\n",
       "        -1.23812175, -1.39345139]),\n",
       " array([ 0.78116361,  0.62583397,  0.47050432,  0.31517468,  0.15984504,\n",
       "         0.0045154 , -0.15081425, -0.30614389, -0.46147353, -0.61680317,\n",
       "        -0.77213282, -0.92746246, -1.0827921 , -1.23812175, -1.39345139]),\n",
       " array([ 0.0045154 , -0.15081425, -0.30614389, -0.46147353, -0.61680317,\n",
       "        -0.77213282, -0.92746246, -1.0827921 , -1.23812175, -1.39345139]),\n",
       " array([ 0.78116361,  0.62583397,  0.47050432,  0.31517468,  0.15984504,\n",
       "         0.0045154 , -0.15081425, -0.30614389, -0.46147353, -0.61680317,\n",
       "        -0.77213282, -0.92746246, -1.0827921 , -1.23812175, -1.39345139]),\n",
       " array([ 2.95577861,  2.80044897,  2.64511932,  2.48978968,  2.33446004,\n",
       "         2.1791304 ,  2.02380075,  1.86847111,  1.71314147,  1.55781182,\n",
       "         1.40248218,  1.24715254,  1.0918229 ,  0.93649325,  0.78116361,\n",
       "         0.62583397,  0.47050432,  0.31517468,  0.15984504,  0.0045154 ,\n",
       "        -0.15081425, -0.30614389, -0.46147353, -0.61680317, -0.77213282,\n",
       "        -0.92746246, -1.0827921 , -1.23812175, -1.39345139]),\n",
       " array([ 0.47050432,  0.31517468,  0.15984504,  0.0045154 , -0.15081425,\n",
       "        -0.30614389, -0.46147353, -0.61680317, -0.77213282, -0.92746246,\n",
       "        -1.0827921 , -1.23812175, -1.39345139]),\n",
       " array([ 1.55781182,  1.40248218,  1.24715254,  1.0918229 ,  0.93649325,\n",
       "         0.78116361,  0.62583397,  0.47050432,  0.31517468,  0.15984504,\n",
       "         0.0045154 , -0.15081425, -0.30614389, -0.46147353, -0.61680317,\n",
       "        -0.77213282, -0.92746246, -1.0827921 , -1.23812175, -1.39345139]),\n",
       " array([ 1.0918229 ,  0.93649325,  0.78116361,  0.62583397,  0.47050432,\n",
       "         0.31517468,  0.15984504,  0.0045154 , -0.15081425, -0.30614389,\n",
       "        -0.46147353, -0.61680317, -0.77213282, -0.92746246, -1.0827921 ,\n",
       "        -1.23812175, -1.39345139]),\n",
       " array([ 1.0918229 ,  0.93649325,  0.78116361,  0.62583397,  0.47050432,\n",
       "         0.31517468,  0.15984504,  0.0045154 , -0.15081425, -0.30614389,\n",
       "        -0.46147353, -0.61680317, -0.77213282, -0.92746246, -1.0827921 ,\n",
       "        -1.23812175, -1.39345139])]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_final_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe0f8c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
